{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72044ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:11:03.039 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:11:03.045 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:11:03.047 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2025-04-27 18:11:03.048 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:11:03.876 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\amro8\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-27 18:11:03.877 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:11:03.878 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:11:04.389 Thread 'Thread-3': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-27 18:11:04.398 Thread 'Thread-3': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import pyodbc\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'server': 'DESKTOP-O0HMMLT',\n",
    "    'database': 'DS_Tools_Project',\n",
    "    'driver': 'ODBC Driver 17 for SQL Server'\n",
    "}\n",
    "\n",
    "# Set page config for Streamlit\n",
    "st.set_page_config(\n",
    "    page_title=\"Books Analysis Dashboard\",\n",
    "    page_icon=\"ðŸ“š\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create and return a database connection.\"\"\"\n",
    "    conn_str = (\n",
    "        f\"DRIVER={DB_CONFIG['driver']};\"\n",
    "        f\"SERVER={DB_CONFIG['server']};\"\n",
    "        f\"DATABASE={DB_CONFIG['database']};\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "    )\n",
    "    return pyodbc.connect(conn_str)\n",
    "\n",
    "\n",
    "def scrape_books(pages_to_scrape=2):\n",
    "    \"\"\"Scrape book data from books.toscrape.com.\"\"\"\n",
    "    base_url = \"http://books.toscrape.com/\"\n",
    "    all_books = []\n",
    "\n",
    "    for page in range(1, pages_to_scrape + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}catalogue/page-{page}.html\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        books = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "        for book in books:\n",
    "            title = book.h3.a['title']\n",
    "            price = book.find('p', class_='price_color').text\n",
    "            rating = book.p['class'][1]\n",
    "\n",
    "            # Robust availability extraction\n",
    "            availability_text = book.find('p', class_='instock availability').text.strip()\n",
    "            availability_match = re.search(r'\\((\\d+) available\\)', availability_text)\n",
    "            availability = int(availability_match.group(1)) if availability_match else (\n",
    "                1 if 'In stock' in availability_text else 0)\n",
    "\n",
    "            book_url = base_url + 'catalogue/' + book.h3.a['href']\n",
    "            book_response = requests.get(book_url)\n",
    "            book_soup = BeautifulSoup(book_response.text, 'html.parser')\n",
    "\n",
    "            description = book_soup.find('meta', attrs={'name': 'description'})['content'].strip()\n",
    "            upc = book_soup.find('th', string='UPC').find_next_sibling('td').text\n",
    "            product_type = book_soup.find('th', string='Product Type').find_next_sibling('td').text\n",
    "            price_excl_tax = book_soup.find('th', string='Price (excl. tax)').find_next_sibling('td').text\n",
    "            price_incl_tax = book_soup.find('th', string='Price (incl. tax)').find_next_sibling('td').text\n",
    "            tax = book_soup.find('th', string='Tax').find_next_sibling('td').text\n",
    "            num_reviews = book_soup.find('th', string='Number of reviews').find_next_sibling('td').text\n",
    "\n",
    "            all_books.append({\n",
    "                'title': title,\n",
    "                'price': price,\n",
    "                'rating': rating,\n",
    "                'availability': availability,\n",
    "                'upc': upc,\n",
    "                'product_type': product_type,\n",
    "                'price_excl_tax': price_excl_tax,\n",
    "                'price_incl_tax': price_incl_tax,\n",
    "                'tax': tax,\n",
    "                'num_reviews': num_reviews,\n",
    "                'description': description\n",
    "            })\n",
    "\n",
    "            time.sleep(0.5)  # Be polite with delay between requests\n",
    "            \n",
    "\n",
    "    return pd.DataFrame(all_books)\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess the scraped book data.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Handle null values\n",
    "    critical_columns = ['price', 'title', 'rating']\n",
    "    df = df.dropna(subset=critical_columns)\n",
    "    df = df.assign(\n",
    "        description=df['description'].fillna('No description'),\n",
    "        num_reviews=df['num_reviews'].fillna(0)\n",
    "    )\n",
    "\n",
    "    # Handle duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Clean price columns\n",
    "    price_columns = ['price', 'price_excl_tax', 'price_incl_tax', 'tax']\n",
    "    for col in price_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: float(re.sub(r'[^\\d.]', '', str(x))) if pd.notnull(x) else 0.0)\n",
    "\n",
    "    # Convert rating to numerical (1-5)\n",
    "    rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "    df['rating'] = df['rating'].map(rating_map).fillna(3)\n",
    "\n",
    "    # Ensure availability is integer\n",
    "    df['availability'] = df['availability'].astype(int)\n",
    "\n",
    "    # Clean description\n",
    "    df['description'] = df['description'].str.strip()\n",
    "\n",
    "    # Convert num_reviews to integer\n",
    "    df['num_reviews'] = df['num_reviews'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_data(df):\n",
    "    \"\"\"Perform basic analysis on the cleaned data.\"\"\"\n",
    "    analysis_results = {\n",
    "        'average_price': df['price'].mean(),\n",
    "        'max_price': df['price'].max(),\n",
    "        'min_price': df['price'].min(),\n",
    "        'total_available': df['availability'].sum(),\n",
    "        'rating_distribution': df['rating'].value_counts().sort_index(),\n",
    "        'price_by_rating': df.groupby('rating')['price'].mean()\n",
    "    }\n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "def save_to_database(df):\n",
    "    \"\"\"Save cleaned data to SQL Server database.\"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create table if it doesn't exist\n",
    "        cursor.execute(\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='Books' AND xtype='U')\n",
    "        CREATE TABLE Books (\n",
    "            ID INT IDENTITY(1,1) PRIMARY KEY,\n",
    "            Title NVARCHAR(255),\n",
    "            Price FLOAT,\n",
    "            Rating INT,\n",
    "            Availability INT,\n",
    "            UPC NVARCHAR(50),\n",
    "            ProductType NVARCHAR(100),\n",
    "            PriceExclTax FLOAT,\n",
    "            PriceInclTax FLOAT,\n",
    "            Tax FLOAT,\n",
    "            NumReviews INT,\n",
    "            Description NVARCHAR(MAX),\n",
    "            DateAdded DATETIME DEFAULT GETDATE()\n",
    "        )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "        # Insert data\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Books (\n",
    "                    Title, Price, Rating, Availability, UPC, ProductType,\n",
    "                    PriceExclTax, PriceInclTax, Tax, NumReviews, Description\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                           row['title'], row['price'], row['rating'], row['availability'],\n",
    "                           row['upc'], row['product_type'], row['price_excl_tax'],\n",
    "                           row['price_incl_tax'], row['tax'], row['num_reviews'],\n",
    "                           row['description']\n",
    "                           )\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Successfully inserted {len(df)} records into the database\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def show_dashboard(cleaned_df):\n",
    "    \"\"\"Display the Streamlit dashboard with the cleaned data.\"\"\"\n",
    "    st.title(\"ðŸ“š Books to Scrape Analysis Dashboard\")\n",
    "    st.markdown(\"\"\"\n",
    "    This interactive dashboard analyzes book data scraped from [books.toscrape.com](http://books.toscrape.com/).\n",
    "    Explore pricing, ratings, availability, and more!\n",
    "    \"\"\")\n",
    "\n",
    "    # Sidebar controls\n",
    "    st.sidebar.header(\"Controls\")\n",
    "    pages_to_scrape = st.sidebar.slider(\"Number of pages to scrape\", 1, 5, 2)\n",
    "    scrape_button = st.sidebar.button(\"Scrape Fresh Data\")\n",
    "\n",
    "    if scrape_button:\n",
    "        with st.spinner(\"Scraping fresh data...\"):\n",
    "            books_df = scrape_books(pages_to_scrape)\n",
    "            cleaned_df = clean_data(books_df)\n",
    "            save_to_database(cleaned_df)\n",
    "            st.session_state.cleaned_df = cleaned_df\n",
    "            st.success(\"Data successfully scraped, cleaned, and saved to database!\")\n",
    "\n",
    "    # Dashboard sections\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\"ðŸ“Š Overview\", \"ðŸ“ˆ Visual Analysis\", \"ðŸ“š Book Explorer\", \"ðŸ’¾ Data\"])\n",
    "\n",
    "    with tab1:\n",
    "        st.header(\"Dataset Overview\")\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        with col1:\n",
    "            st.metric(\"Total Books\", len(cleaned_df))\n",
    "        with col2:\n",
    "            st.metric(\"Average Price\", f\"Â£{cleaned_df['price'].mean():.2f}\")\n",
    "        with col3:\n",
    "            st.metric(\"Total Available\", cleaned_df['availability'].sum())\n",
    "        with col4:\n",
    "            st.metric(\"Average Rating\", f\"{cleaned_df['rating'].mean():.1f} â˜…\")\n",
    "\n",
    "        st.subheader(\"Sample Data\")\n",
    "        st.dataframe(cleaned_df.head(10), use_container_width=True)\n",
    "\n",
    "    with tab2:\n",
    "        st.header(\"ðŸ“Š Visual Analysis\")\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.subheader(\"Price Distribution\")\n",
    "            fig1 = plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(cleaned_df['price'], bins=20, kde=True)\n",
    "            st.pyplot(fig1)\n",
    "\n",
    "        with col2:\n",
    "            st.subheader(\"Rating Distribution\")\n",
    "            fig2 = plt.figure(figsize=(10, 6))\n",
    "            sns.countplot(x='rating', data=cleaned_df)\n",
    "            st.pyplot(fig2)\n",
    "\n",
    "        col3, col4 = st.columns(2)\n",
    "        with col3:\n",
    "            st.subheader(\"Price by Rating\")\n",
    "            fig3 = plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(x='rating', y='price', data=cleaned_df)\n",
    "            st.pyplot(fig3)\n",
    "\n",
    "        with col4:\n",
    "            st.subheader(\"Availability vs Price\")\n",
    "            fig4 = plt.figure(figsize=(10, 6))\n",
    "            sns.scatterplot(x='availability', y='price', data=cleaned_df, hue='rating')\n",
    "            st.pyplot(fig4)\n",
    "\n",
    "    with tab3:\n",
    "        st.header(\"Book Explorer\")\n",
    "        search_term = st.text_input(\"Search books by title or description\")\n",
    "        rating_filter = st.multiselect(\n",
    "            \"Filter by rating\",\n",
    "            options=sorted(cleaned_df['rating'].unique()),\n",
    "            default=sorted(cleaned_df['rating'].unique())\n",
    "        )\n",
    "\n",
    "        price_range = st.slider(\n",
    "            \"Price range (Â£)\",\n",
    "            float(cleaned_df['price'].min()),\n",
    "            float(cleaned_df['price'].max()),\n",
    "            (float(cleaned_df['price'].min()), float(cleaned_df['price'].max()))\n",
    "        )\n",
    "\n",
    "        filtered_df = cleaned_df[\n",
    "            (cleaned_df['price'] >= price_range[0]) &\n",
    "            (cleaned_df['price'] <= price_range[1]) &\n",
    "            (cleaned_df['rating'].isin(rating_filter))\n",
    "            ]\n",
    "\n",
    "        if search_term:\n",
    "            filtered_df = filtered_df[\n",
    "                filtered_df['title'].str.contains(search_term, case=False) |\n",
    "                filtered_df['description'].str.contains(search_term, case=False)\n",
    "                ]\n",
    "\n",
    "        st.subheader(f\"Found {len(filtered_df)} books matching your criteria\")\n",
    "\n",
    "        for _, row in filtered_df.iterrows():\n",
    "            with st.expander(f\"{row['title']} - Â£{row['price']:.2f} | Rating: {'â˜…' * row['rating']}\"):\n",
    "                col1, col2 = st.columns([1, 3])\n",
    "                with col1:\n",
    "                    st.metric(\"Price\", f\"Â£{row['price']:.2f}\")\n",
    "                    st.metric(\"Rating\", f\"{row['rating']} â˜…\")\n",
    "                    st.metric(\"Available\", row['availability'])\n",
    "                with col2:\n",
    "                    st.write(f\"**Description:** {row['description']}\")\n",
    "                    st.caption(f\"**Product Type:** {row['product_type']} | **Reviews:** {row['num_reviews']}\")\n",
    "\n",
    "    with tab4:\n",
    "        st.header(\"Raw Data\")\n",
    "        st.download_button(\n",
    "            label=\"Download Cleaned Data as CSV\",\n",
    "            data=cleaned_df.to_csv(index=False).encode('utf-8'),\n",
    "            file_name='cleaned_books_data.csv',\n",
    "            mime='text/csv'\n",
    "        )\n",
    "        st.dataframe(cleaned_df, use_container_width=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Check if data exists in session state\n",
    "    if 'cleaned_df' not in st.session_state:\n",
    "        # Scrape, clean and load initial data\n",
    "        with st.spinner(\"Loading initial data...\"):\n",
    "            books_df = scrape_books()\n",
    "            cleaned_df = clean_data(books_df)\n",
    "            save_to_database(cleaned_df)\n",
    "            st.session_state.cleaned_df = cleaned_df\n",
    "\n",
    "    # Show the dashboard\n",
    "    show_dashboard(st.session_state.cleaned_df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
